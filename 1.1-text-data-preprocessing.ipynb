{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing of Text Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Diary-style Text Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "1. [journal-entries-with-labelled-emotions](https://www.kaggle.com/datasets/madhavmalhotra/journal-entries-with-labelled-emotions) — 1500 journal entries labelled with 18 emotions from people reflecting on their day.\n",
    "2. [Diary-Entry-To-Rap](https://huggingface.co/datasets/chaudha7/Diary-Entry-To-Rap) — 175 diary entries with age and gender writer's info plus transformed diary into rap text with certain mood and style.\n",
    "\n",
    "From both datasets only texts will be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read only diary-style text entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Answer.f1.afraid.raw</th>\n",
       "      <th>Answer.f1.angry.raw</th>\n",
       "      <th>Answer.f1.anxious.raw</th>\n",
       "      <th>Answer.f1.ashamed.raw</th>\n",
       "      <th>Answer.f1.awkward.raw</th>\n",
       "      <th>Answer.f1.bored.raw</th>\n",
       "      <th>Answer.f1.calm.raw</th>\n",
       "      <th>Answer.f1.confused.raw</th>\n",
       "      <th>Answer.f1.disgusted.raw</th>\n",
       "      <th>...</th>\n",
       "      <th>Answer.t1.family.raw</th>\n",
       "      <th>Answer.t1.food.raw</th>\n",
       "      <th>Answer.t1.friends.raw</th>\n",
       "      <th>Answer.t1.god.raw</th>\n",
       "      <th>Answer.t1.health.raw</th>\n",
       "      <th>Answer.t1.love.raw</th>\n",
       "      <th>Answer.t1.recreation.raw</th>\n",
       "      <th>Answer.t1.school.raw</th>\n",
       "      <th>Answer.t1.sleep.raw</th>\n",
       "      <th>Answer.t1.work.raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family was the most salient part of my day,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer  Answer.f1.afraid.raw  \\\n",
       "0  My family was the most salient part of my day,...                 False   \n",
       "\n",
       "   Answer.f1.angry.raw  Answer.f1.anxious.raw  Answer.f1.ashamed.raw  \\\n",
       "0                False                   True                  False   \n",
       "\n",
       "   Answer.f1.awkward.raw  Answer.f1.bored.raw  Answer.f1.calm.raw  \\\n",
       "0                  False                False               False   \n",
       "\n",
       "   Answer.f1.confused.raw  Answer.f1.disgusted.raw  ...  Answer.t1.family.raw  \\\n",
       "0                   False                    False  ...                  True   \n",
       "\n",
       "   Answer.t1.food.raw  Answer.t1.friends.raw  Answer.t1.god.raw  \\\n",
       "0               False                  False              False   \n",
       "\n",
       "   Answer.t1.health.raw  Answer.t1.love.raw  Answer.t1.recreation.raw  \\\n",
       "0                 False               False                     False   \n",
       "\n",
       "   Answer.t1.school.raw  Answer.t1.sleep.raw  Answer.t1.work.raw  \n",
       "0                 False                False               False  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "journals_ds_dir = 'data/journals.csv'\n",
    "\n",
    "journals_df = pd.read_csv(journals_ds_dir)\n",
    "journals_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1473,\n",
       " ['My family was the most salient part of my day, since most days the care of my 2 children occupies the majority of my time. They are 2 years old and 7 months and I love them, but they also require so much attention that my anxiety is higher than ever. I am often overwhelmed by the care the require, but at the same, I am so excited to see them hit developmental and social milestones.',\n",
       "  'Yoga keeps me focused. I am able to take some time for me and breath and work my body. This is important because it sets up my mood for the whole day.',\n",
       "  'Yesterday, my family and I played a bunch of board games. My husband won most of them which is not surprising in the least. We played all sorts of games including Life, Clue, Mouse Trap and more. It was relaxing and such a happy, fun filled moment.',\n",
       "  \"Yesterday, I visited my parents and had dinner with them.  I hadn't seen them in a few weeks, so it was wonderful to see them and catch up on things.\",\n",
       "  'Yesterday, I really felt the importance of my health. I went on a bit longer hike than usual and was very happy that I could do so. It really made me appreciate my health. With all the news of people dying and the tragedy of the Utah family murder in Mexico, it really made me aware of the importance of my own health and well being.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journals = journals_df['Answer'].to_list()\n",
    "len(journals), journals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'Age', 'Gender', 'Mood', 'Writing Manner', 'Diary Entry', 'Rap Song'],\n",
       "        num_rows: 175\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "diaries_ds = load_dataset('chaudha7/Diary-Entry-To-Rap')\n",
    "diaries_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175,\n",
       " ['Dear Diary,\\n\\nI find solace in your pages today, as my weary soul seeks refuge from the relentless demands of the world. Fatigue has woven its heavy tendrils around my body, stifling my spirit and clouding my mind. The weariness seems never-ending, suffocating every ounce of energy that once resided within me.\\n\\nThe incessant noise and constant motion of life have suffused my being, leaving me utterly drained. The weight of expectations and responsibilities bears down upon my shoulders, making each step feel like a monumental effort. It feels as if I am eternally trapped in a labyrinth of tasks, forever searching for an escape that eludes me.\\n\\nIn this moment, all I desire is solitude; a chance to retreat into the sanctuary of my introverted nature. To find solace within myself and reconnect with the depths of my being. The mere thought of self-care becomes a distant dream, fading like a mirage in the scorching desert of my exhaustion.\\n\\nPerhaps tomorrow will bring respite, allowing my weary soul to rejuvenate. Until then, I shall cling to the hope that the storm inside me will dissipate, and I will once again feel the gentle touch of serenity.\\n\\nYours,\\n[A Weary Soul]',\n",
       "  \"Dear Diary,\\n\\nIt feels as though my heart has been shattered into a million pieces, each shard penetrating my soul with a pain so unbearable. The weight of this heartbreak drags me down, rendering me helpless against the tsunami of emotions crashing over me. At forty-two, I thought I had experienced all of life's hardships, but nothing could have prepared me for this gut-wrenching agony.\\n\\nEvery moment feels like a never-ending echo of her absence. The memories we shared, once cherished, now lurk in the recesses of my mind, torturing me. The laughter, the gentle touch, and the promises of a lifetime together—all dashed into oblivion. How could love, once so tender and nurturing, turn into a brutal hurricane that leaves me standing amidst the wreckage?\\n\\nMy days blend into an incoherent blur, lacking joy or purpose. The mundane tasks I once found solace in now mock me, reminding me of the void she left behind. How can life move forward when my heart still resides in the past?\\n\\nBut as I write these words, a faint glimmer of hope flickers deep within. Perhaps, in time, this heartbreak will soften its grip and allow me to breathe again. Until then, I mourn the loss of a love that once defined me.\\n\\nHeavy-hearted,\\n[A Devastated Soul]\",\n",
       "  \"Dear Diary,\\n\\nI'm absolutely delighted to begin today's entry with an overwhelming sense of happiness and contentment. It's truly a remarkable feeling! Everything around me seems to sparkle with joy, and the world appears so vibrant and full of possibilities.\\n\\nThis morning, as the sun greeted me with its warm rays, I couldn't help but reflect on all the positive aspects in my life. Whether it's the unwavering support of my family, the cherished friendships I've fostered, or the knowledge I've gained in my incredible school, I'm immensely grateful.\\n\\nNot only am I surrounded by wonderful people, but I've also been blessed with an array of hobbies and interests that bring me immense joy. From tinkering with electronic gadgets to exploring the depths of fictional worlds through books, these pursuits make every day an exciting adventure.\\n\\nMoreover, the simple moments in life elevate my spirits further. The comforting aroma of freshly baked cookies, the soothing melody of birdsong outside my window, or the sight of a blossoming flower—all of these gentle reminders of beauty fill me with inexplicable delight.\\n\\nToday, I choose to embrace my happy state of mind, for it allows me to approach each encounter and endeavor with enthusiasm and optimism. Let this joyous bubble of happiness continue to inspire gratitude and motivate me to spread positivity wherever I go.\\n\\nWith a heart brimming with happiness,\\n\\n[Your Name]\",\n",
       "  \"Dear Diary,\\n\\nWhat an absolutely splendid day it has been! I must say, today I feel like a young lad again, brimming with joy and vitality. The sun shone brighter this morning, casting its golden rays on the world, as if to mirror the happiness that radiates from within me. Oh, the feeling of warm sunlight rejuvenating my spirit!\\n\\nAs I sipped my morning coffee, every sip seemed to dance on my tongue, releasing a burst of flavor that tingled with delight. The vibrant aroma filled the air, mingling with the melodious chirping of the birds, creating a symphony that echoed through my heart. It felt as if the very essence of life was pulsating through my senses, awakening every dormant cell.\\n\\nThroughout the day, laughter seemed to flow effortlessly from my lips, as if contagious. Each interaction—be it with a colleague, a friend, or even a stranger—left me grinning from ear to ear. The world suddenly seemed so beautiful and full of promise, as I reveled in life's simple pleasures.\\n\\nTonight, as I retire to my humble abode, my heart is filled with gratitude for this glorious day. Oh, how fortunate I am to be engulfed in such contentment and bliss. Life truly is a beautiful journey, and today, I am going to bed with a soul that is brimming with joy.\\n\\nTill tomorrow, dear Diary, may the happiness within me continue to sparkle like a radiant star in the night sky.\\n\\nYours joyfully,\\n[Name]\",\n",
       "  \"Dear Diary,\\n\\nToday has been a rather delightful day. The sun's warm rays gently illuminated my room, coaxing me out of bed with a sense of contentment. As I sipped my morning coffee, a profound sense of gratitude washed over me, and I couldn't help but smile.\\n\\nThe simplicity of this day has brought me immense joy. The solitude I find myself in has become a cherished companion, allowing me to fully appreciate the little things in life that often go unnoticed. The soft hum of nature outside my window, the familiar scent of fresh air, these simple pleasures always manage to nudge my heart.\\n\\nI've had the opportunity to lose myself in the pages of a captivating book, escaping to distant lands and immersing myself in otherworldly adventures. As the hours slipped away, I felt an indescribable satisfaction, as if I had discovered a hidden treasure within the depths of my quiet sanctuary.\\n\\nReflecting on this serene day, I realize that happiness can be found in the calmest of moments. In my solitude, I have discovered a blissful solitude that ignites a profound sense of fulfillment within me. The world may continue its relentless hustle and bustle, but for now, I find solace in the tranquil embrace of my own company.\\n\\nUntil tomorrow,\\n[Name]\"])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diaries = diaries_ds['train']['Diary Entry']\n",
    "len(diaries), diaries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diary_texts = journals + diaries\n",
    "len(diary_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion-multilabel-latest.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/vmakh/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "reddit_classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "twitter_classifier = pipeline(task=\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFRobertaEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 514 is not in [0, 514) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFRobertaEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 514), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(1, 514), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/vmakh/IU/PMLDL/quotes-recsys/1.1-text-data-preprocessing.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vmakh/IU/PMLDL/quotes-recsys/1.1-text-data-preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m reddit_outputs \u001b[39m=\u001b[39m reddit_classifier(diary_texts)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vmakh/IU/PMLDL/quotes-recsys/1.1-text-data-preprocessing.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m twitter_outputs \u001b[39m=\u001b[39m twitter_classifier(diary_texts)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1124\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_multi(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1125\u001b[0m \u001b[39melif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1127\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1128\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1143\u001b[0m, in \u001b[0;36mPipeline.run_multi\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_multi\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(item, preprocess_params, forward_params, postprocess_params) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m inputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_multi\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(item, preprocess_params, forward_params, postprocess_params) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m inputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1041\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1040\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1042\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1043\u001b[0m     inference_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_inference_context()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_tf_roberta.py:1277\u001b[0m, in \u001b[0;36mTFRobertaForSequenceClassification.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m   1249\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(ROBERTA_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1250\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1270\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m   1271\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[39m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[39m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[39m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1277\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1278\u001b[0m         input_ids,\n\u001b[1;32m   1279\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1280\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1281\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1282\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1283\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1284\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1285\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1286\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1287\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[1;32m   1289\u001b[0m     sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1290\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output, training\u001b[39m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_tf_roberta.py:654\u001b[0m, in \u001b[0;36mTFRobertaMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     token_type_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfill(dims\u001b[39m=\u001b[39minput_shape, value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 654\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    655\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    656\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    657\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    658\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    659\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    660\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    661\u001b[0m )\n\u001b[1;32m    663\u001b[0m \u001b[39m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[39m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[39m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m attention_mask_shape \u001b[39m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_tf_roberta.py:167\u001b[0m, in \u001b[0;36mTFRobertaEmbeddings.call\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         position_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(\n\u001b[1;32m    164\u001b[0m             tf\u001b[39m.\u001b[39mrange(start\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, limit\u001b[39m=\u001b[39minput_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    165\u001b[0m         )\n\u001b[0;32m--> 167\u001b[0m position_embeds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mgather(params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embeddings, indices\u001b[39m=\u001b[39;49mposition_ids)\n\u001b[1;32m    168\u001b[0m token_type_embeds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgather(params\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings, indices\u001b[39m=\u001b[39mtoken_type_ids)\n\u001b[1;32m    169\u001b[0m final_embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds \u001b[39m+\u001b[39m token_type_embeds\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFRobertaEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 514 is not in [0, 514) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFRobertaEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 514), dtype=int32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(1, 514), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False"
     ]
    }
   ],
   "source": [
    "reddit_outputs = reddit_classifier(diary_texts)\n",
    "twitter_outputs = twitter_classifier(diary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reddit_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vmakh/IU/PMLDL/quotes-recsys/1.1-text-data-preprocessing.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vmakh/IU/PMLDL/quotes-recsys/1.1-text-data-preprocessing.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m reddit_outputs[:\u001b[39m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reddit_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "reddit_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_output_into_lists(output):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diaries_reddit_classified = pd.DataFrame(diary_texts reddit_labels, columns=)\n",
    "# diaries_twitter_classified = ...s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reddit and Twitter Posts Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "1. [go_emotions](https://huggingface.co/datasets/go_emotions) — 58k carefully curated Reddit comments labeled for 27 emotion categories or Neutral.\n",
    "2. [sem_eval_2018_task_1](https://huggingface.co/datasets/sem_eval_2018_task_1) — a dataset of Twitter tweets in Arabic, English (11k entries), Spanish to automatically determine the intensity of emotions (E) and intensity of sentiment (aka valence V) of the tweeters from their tweets.\n",
    "\n",
    "Both datasets are designed to solve multi-label emotion classification task. From both datasets only texts will be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
